# Paper_Path

This is a repo for important papers that shape my scientific views of computational neuroscience and neuroengineering (especially Brain Computer Interfacing).


### Population Dynamics
1) __Neural Population Dynamics during Reaching__: Ever since [Mark Churchland 2012](https://www.nature.com/articles/nature11129) (introducing jPCA, rotations) who found out the unexpected rotational structures as the underlying topology of reaching behaviors, population studies on dynamics of neural data have been put into mainstream neural data analysis.


2) __Neural Constraints on Learning__  [Patrick Sadtler 2014](https://www.nature.com/articles/nature13665) (FA+Kalman decompositions, their intrinsic space definitions (CMU/Pittsburgh is different from BrainGate), Byron Yu et al. used a different approach to estimate the intrinsic manifold (not transforming the data but using the data to find out the intrinsic manifold) _Thoughts: perturbations within and outside ---- seems like an intermediary zone between science and engineering_ ). But the issue I personally think of with CMU/Pittsburgh models is that the latent factors/intrinsic space has no temporal information in it. But one key idea of the "intuitive mapping" is significant: we will NOT directly map between neural activities and cursor kinematics!!! Instead between them there's a layer of latent factors. It's by such separation that we now have two layers that the researchers eventually could conduct 2 kinds of perturbations.

3) __Neural Trajectories in the Supplementary Motor Area and Motor Cortex Exhibit Distinct Geometries, Compatible with Different Classes of Computation__: 
[Russo and Churchland 2020](https://www.sciencedirect.com/science/article/pii/S0896627320303664) studied the behavior of supplementary motor area (SMA) under the central hypothesis that SMA computations would internally track motion context (the choice of the cycle task is clever). They used low dimensional projections to illustrate the differences in trajectories of M1 and SMA and analyzed the subspace overlaps the different projections took. Specifically, they brought up the method of neural trajectory divergence and calculated that SMA trajectories had low divergence as expected. It's also inspiring that they used ANN and simulations to corroborate their conclusions. In the end it's enjoyment to read their discussions whether dynamical systems view would be interpreted with any assigned representations.

4) __Learning identifiable and interpretable latent models of high-dimensional neural activity using pi-VAE__ : [Zhou et al 2020](https://arxiv.org/abs/2011.04798) 's approach of hybrid models (label --- latent --- observation) is interesting though not too much science corroborated. The enlightenment this paper endows is its finding that its model pi-VAE seems to find subspaces which encode movement directions (spatial information) and neural dynamics (temporal evolution) by visualizing different combinations latent dimensions, which adds to the debate of whether the low dimensional latent space indeed "represents" anything. It also mentions GIN (General Incompressible-flow Network)

5) __Modeling behaviorally relevant neural dynamics enabled by preferential subspace identification__ [Sani et al 2021](https://www.nature.com/articles/s41593-020-00733-0): They developed a new model called PSID, which uses both neural activity and behavior in training data and prioritizes extracting behaviorally relevant neural dynamics. It turns out out that PSID learned latent factors with much less dimensions and encode behaviorally related latent dimensions with higher accuracy with fewer training samples. Surprisingly, PSID finds oppositely orientated latent trajectories for reach and return movements while other models encode the trajectories with the same orientation (Sup video 1 for further reflections). Towards the end, the authors also talked about the potential usage of PSID even to two different sources of brain signals to extract common dynamics! Be sure to understand the logic (the idea of projection and SVD to find behaviorally related representations!!) of PSID model! This paper reminds again to 1) focus on block design of models; 2) reflect deep on the relationships among neural signals, latent states, and behaviors

6) __Dynamical flexible inference of nonlinear latent factors and structures in neural population activity__ [Abbaspourazad 2023](https://www.nature.com/articles/s41551-023-01106-1) devised the DFINE mode which could do flexible inference (both noncausal smoothing and causal filtering) with missing observation values. Specifically, this model achieves better neural and behavioral predictions results and learned more robustly the underlying manifolds. The key point of the paper is that DFINE makes a two-layer structure: a nonlinear manifold latent factor, and an underlying linear dynamical latent factor, and train them together. The reflection is that we should think in terms of blocks of model and do forceful alignment.

7) __Expressive dynamics models with nonlinear injective readouts enable reliable recovery of latent features from neural activity__ [Versteeg et al 2023](https://pubmed.ncbi.nlm.nih.gov/37744459/#:~:text=Expressive%20dynamics%20models%20with%20nonlinear,latent%20features%20from%20neural%20activity) developed a model they called ODIN which conducts approximately injective mapping from latent space to neural space (so all changes in latent dynamics would necessarily affect neural states). This is specifically done by importing Neural ODE and techniques similar to invertible ResNets. Implementation of normalizing flows ensure the "approximate injectivity". Notice that unlike LFADS, ODIN cannot take non-autonomous inputs. This paper first introduces me to normalizing flows (injectivity) and I see the first time Neural ODE applied in population latent dynamics models. Furthermore, ODIN uses three different implementations for dynamics and mapping, which illustrate how making a model is similar to building Lego blocks.



### Review Papers
1) __Neural Manifolds for the Control of Movement__: [Gallego et al 2017](https://pubmed.ncbi.nlm.nih.gov/28595054/) emphasized 1) the connections between neural connectivity and the constrained and thus low dimensional latent variable manifold; 2) neural modes in motor cortices(1] population variability in PMd is reduced by stimulus presentation, 2] the timeline of studies on the orthogonal potent/null spaces for movement and preparatory activities); 3) learning and connectivity which affect the latent manifold (1] CMU Sadtler & Byron Yu perturbation and learning within/outside of latent space, 2]Sussilo training RNN to represent learning, 3] balanced neural networks also exhibited oscillatory behaviors)

2) __Interpreting neural computations by examining intrinsic and embedding dimensionality of neural activity__ [Jazayeri, Ostojic 2021](https://www.sciencedirect.com/science/article/pii/S0959438821000933) wrote a review paper. Specifically, they differentiated embedding dimensionality and intrinsic dimensionality, the former encoding the way all the sensory and movement inputs are processed, while the latter encoding the latent variables themselves. Notice the paper's discussions on orthogonal subspace, model generalization, and controllability. It's also worth attention how the authors emphasized the role of recurrent dynamics in latent feature extractions. At the end, the authors briefly mentioned methods for model reductions: 1) Model distillation; 2) Mean-field analysis (especially to directly extract latent variables from RNN!)

3) __Preparatory activity and the expansive null-space__ [Churchland 2024](https://www.nature.com/articles/s41583-024-00796-z) illustrated Churchland's thoughts on the distinction between output-potent and output-null subspaces. They theorized on how rotatory behaviors might be explained from the perspective of the null-space theory (rotations give the null-space possibility to handle preparatory activities), and how cycling might reduce neural entangling. The flexibility-via-subspace hypothesis is informative for experiment design and hypothesis generations.



